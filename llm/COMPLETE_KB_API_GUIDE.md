# Complete Knowledge Base API Guide

**Project:** LLM4Reqs Knowledge Base API  
**Version:** 1.0.0  
**Last Updated:** October 20, 2025  
**Status:** ‚úÖ Production Ready

---

## Table of Contents

1. [Quick Start (5 Minutes)](#quick-start)
2. [API Endpoints Reference](#api-endpoints)
3. [Authentication Explained](#authentication)
4. [Integration Guide](#integration)
5. [Configuration](#configuration)
6. [Testing](#testing)
7. [Troubleshooting](#troubleshooting)
8. [Production Deployment](#production)

---

## Quick Start

### Prerequisites
- Python 3.8+
- pip installed
- Git repository cloned

### Setup (5 Minutes)

```powershell
# 1. Navigate to LLM directory
cd llm

# 2. Install dependencies
pip install -r requirements.txt

# 3. Create .env file
echo "GROQ_API_KEY=your-groq-api-key-here" > .env
echo "LLM_API_KEY=dev-secret-key-12345" >> .env
echo "KB_BASE_DIR=faiss_store" >> .env
echo "KB_MODEL=all-MiniLM-L6-v2" >> .env

# 4. Start service
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# 5. Test health
curl http://localhost:8000/health
```

### First API Call

```powershell
# Build a knowledge base
curl -X POST "http://localhost:8000/kb/build" `
  -H "X-API-Key: dev-secret-key-12345" `
  -H "Content-Type: application/json" `
  -d '{
    "project_id": "test_project",
    "documents": [
      {"content": "Users must login with email and password", "type": "functional", "meta": {}},
      {"content": "Response time must be under 2 seconds", "type": "non-functional", "meta": {}}
    ],
    "mode": "sync"
  }'

# Query the knowledge base
curl -X POST "http://localhost:8000/kb/query" `
  -H "X-API-Key: dev-secret-key-12345" `
  -H "Content-Type: application/json" `
  -d '{
    "project_id": "test_project",
    "query": "What are the authentication requirements?",
    "top_k": 3
  }'
```

---

## API Endpoints

### 1. POST /process_document

Extract requirements from documents using LLM analysis.

**Request:**
```json
{
  "project_id": "proj_123",
  "document_content": "The system must allow users to login...",
  "document_type": "requirements"
}
```

**Response:**
```json
{
  "project_id": "proj_123",
  "requirements": [
    {
      "id": "REQ-001",
      "content": "User authentication via email and password",
      "type": "functional",
      "priority": "high",
      "source": "document_123"
    }
  ],
  "chunks": [
    {
      "chunk_id": "chunk-1",
      "content": "User authentication...",
      "metadata": {"type": "functional"}
    }
  ],
  "total_requirements": 1,
  "total_chunks": 1
}
```

**PowerShell Example:**
```powershell
$body = @{
    project_id = "proj_123"
    document_content = "Users must login with email and password. Response time under 2 seconds."
    document_type = "requirements"
} | ConvertTo-Json

Invoke-RestMethod -Uri "http://localhost:8000/process_document" `
  -Method POST `
  -Headers @{"X-API-Key" = "dev-secret-key-12345"; "Content-Type" = "application/json"} `
  -Body $body
```

---

### 2. POST /kb/build

Build FAISS vector index for a project.

**Key Features:**
- **Async mode**: Returns `job_id` for long operations
- **Sync mode**: Waits for completion, returns result immediately
- **Project isolation**: Each project gets its own index

**Request:**
```json
{
  "project_id": "proj_123",
  "documents": [
    {
      "content": "User authentication is required",
      "type": "functional",
      "meta": {"priority": "high"}
    },
    {
      "content": "Response time under 2 seconds",
      "type": "non-functional",
      "meta": {"priority": "medium"}
    }
  ],
  "mode": "async"
}
```

**Response (Async):**
```json
{
  "project_id": "proj_123",
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "queued",
  "message": "KB build started"
}
```

**Response (Sync):**
```json
{
  "project_id": "proj_123",
  "status": "completed",
  "index_path": "faiss_store/proj_123/faiss_index.bin",
  "total_chunks": 2
}
```

**Usage Examples:**

```javascript
// JavaScript/React Example
const buildKB = async (projectId, documents) => {
  const response = await fetch('http://localhost:8000/kb/build', {
    method: 'POST',
    headers: {
      'X-API-Key': 'dev-secret-key-12345',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      project_id: projectId,
      documents: documents,
      mode: 'async'
    })
  });
  
  const data = await response.json();
  
  if (data.job_id) {
    // Poll for completion
    const result = await pollJobStatus(data.job_id);
    return result;
  }
  
  return data;
};

const pollJobStatus = async (jobId) => {
  while (true) {
    const response = await fetch(`http://localhost:8000/kb/job/${jobId}`, {
      headers: { 'X-API-Key': 'dev-secret-key-12345' }
    });
    
    const job = await response.json();
    
    if (job.status === 'completed') {
      return job.result;
    } else if (job.status === 'failed') {
      throw new Error(job.error);
    }
    
    // Wait 2 seconds before next poll
    await new Promise(resolve => setTimeout(resolve, 2000));
  }
};
```

```php
// PHP/Laravel Example
class LLMService {
    private $apiKey;
    private $baseUrl;
    
    public function __construct() {
        $this->apiKey = env('LLM_API_KEY');
        $this->baseUrl = env('LLM_SERVICE_URL');
    }
    
    public function buildKB($projectId, $documents, $async = true) {
        $response = Http::withHeaders([
            'X-API-Key' => $this->apiKey,
        ])->post("{$this->baseUrl}/kb/build", [
            'project_id' => $projectId,
            'documents' => $documents,
            'mode' => $async ? 'async' : 'sync'
        ]);
        
        return $response->json();
    }
    
    public function pollJobStatus($jobId) {
        $maxAttempts = 60; // 60 attempts = 2 minutes
        $attempt = 0;
        
        while ($attempt < $maxAttempts) {
            $response = Http::withHeaders([
                'X-API-Key' => $this->apiKey,
            ])->get("{$this->baseUrl}/kb/job/{$jobId}");
            
            $job = $response->json();
            
            if ($job['status'] === 'completed') {
                return $job['result'];
            } else if ($job['status'] === 'failed') {
                throw new Exception($job['error']);
            }
            
            sleep(2);
            $attempt++;
        }
        
        throw new Exception('Job timeout');
    }
}
```

---

### 3. POST /kb/incremental

Add documents to an existing knowledge base without rebuilding.

**Request:**
```json
{
  "project_id": "proj_123",
  "documents": [
    {
      "content": "New requirement: Two-factor authentication",
      "type": "functional",
      "meta": {"priority": "high"}
    }
  ]
}
```

**Response:**
```json
{
  "project_id": "proj_123",
  "status": "success",
  "added_chunks": 1,
  "total_chunks": 51,
  "new_version": 2
}
```

**Error (KB Not Found):**
```json
{
  "detail": "Knowledge base not found for project proj_123. Use /kb/build first."
}
```

---

### 4. POST /kb/query

Semantic search across project knowledge base.

**Request:**
```json
{
  "project_id": "proj_123",
  "query": "What are the authentication requirements?",
  "top_k": 5
}
```

**Response:**
```json
{
  "project_id": "proj_123",
  "query": "What are the authentication requirements?",
  "results": [
    {
      "content": "User authentication via email and password",
      "type": "functional",
      "score": 0.87,
      "metadata": {"priority": "high"}
    },
    {
      "content": "Two-factor authentication required",
      "type": "functional",
      "score": 0.82,
      "metadata": {"priority": "high"}
    }
  ],
  "scores": [0.87, 0.82],
  "total_results": 2
}
```

**JavaScript Example:**
```javascript
const queryKB = async (projectId, query, topK = 5) => {
  const response = await fetch('http://localhost:8000/kb/query', {
    method: 'POST',
    headers: {
      'X-API-Key': 'dev-secret-key-12345',
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      project_id: projectId,
      query: query,
      top_k: topK
    })
  });
  
  return await response.json();
};

// Usage in chat application
const handleUserMessage = async (message, projectId) => {
  // Get relevant context from KB
  const context = await queryKB(projectId, message, 3);
  
  // Pass context to LLM for response generation
  const llmResponse = await generateResponse(message, context.results);
  
  return llmResponse;
};
```

---

### 5. GET /kb/status/{project_id}

Check knowledge base status and metadata.

**Response (KB Exists):**
```json
{
  "project_id": "proj_123",
  "exists": true,
  "version": 2,
  "last_built_at": "2025-10-20T10:30:00Z",
  "total_chunks": 51,
  "error": null
}
```

**Response (KB Not Found):**
```json
{
  "project_id": "proj_456",
  "exists": false,
  "version": 0,
  "last_built_at": null,
  "total_chunks": 0,
  "error": null
}
```

**Usage Example:**
```javascript
// React Component
const KBStatusIndicator = ({ projectId }) => {
  const [status, setStatus] = useState(null);
  
  useEffect(() => {
    const fetchStatus = async () => {
      const response = await fetch(
        `http://localhost:8000/kb/status/${projectId}`,
        { headers: { 'X-API-Key': 'dev-secret-key-12345' } }
      );
      setStatus(await response.json());
    };
    
    fetchStatus();
    const interval = setInterval(fetchStatus, 30000); // Refresh every 30s
    
    return () => clearInterval(interval);
  }, [projectId]);
  
  if (!status) return <div>Loading...</div>;
  
  return (
    <div>
      <h3>Knowledge Base Status</h3>
      <p>Status: {status.exists ? '‚úÖ Active' : '‚ö†Ô∏è Not Built'}</p>
      {status.exists && (
        <>
          <p>Version: {status.version}</p>
          <p>Total Chunks: {status.total_chunks}</p>
          <p>Last Updated: {new Date(status.last_built_at).toLocaleString()}</p>
        </>
      )}
    </div>
  );
};
```

---

### 6. GET /kb/job/{job_id}

Track async build job progress.

**Response (Building):**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "building",
  "project_id": "proj_123",
  "created_at": "2025-10-20T10:30:00Z",
  "result": null,
  "error": null
}
```

**Response (Completed):**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "project_id": "proj_123",
  "created_at": "2025-10-20T10:30:00Z",
  "result": {
    "index_path": "faiss_store/proj_123/faiss_index.bin",
    "total_chunks": 50
  },
  "error": null
}
```

**Response (Failed):**
```json
{
  "job_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "failed",
  "project_id": "proj_123",
  "created_at": "2025-10-20T10:30:00Z",
  "result": null,
  "error": "Failed to build index: Invalid document format"
}
```

---

## Authentication

### Understanding API Keys

**üîë CRITICAL CONCEPT:** There are TWO different authentication systems:

#### System 1: User Authentication (JWT Tokens)
- **Who uses it:** Individual users (Alice, Bob, Carol)
- **Purpose:** Prove "I am Alice"
- **How many:** One per user (thousands)
- **Where stored:** User's browser (localStorage)
- **Created when:** User registers/logs in
- **Example:** `Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...`

#### System 2: Service Authentication (LLM API Key)
- **Who uses it:** Your backend service
- **Purpose:** Prove "I am your authorized backend"
- **How many:** ONE shared key
- **Where stored:** Backend server (.env file)
- **Created when:** By DevOps during deployment
- **Example:** `X-API-Key: dev-secret-key-12345`

### Authentication Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User      ‚îÇ JWT     ‚îÇ   Backend    ‚îÇ API Key ‚îÇ LLM Service ‚îÇ
‚îÇ  (Alice)    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  (Laravel)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  (FastAPI)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ                         ‚îÇ
                        Uses user's JWT           Validates API key
                        to verify identity        to authorize service
```

**Real Example:**

1. **Alice logs in:**
   ```
   POST /api/login
   Body: { email: "alice@company.com", password: "‚Ä¢‚Ä¢‚Ä¢‚Ä¢" }
   Response: { token: "eyJhbGciOi..." }
   ```

2. **Alice queries her project:**
   ```
   POST /api/projects/123/query
   Headers: Authorization: Bearer eyJhbGciOi...
   Body: { query: "authentication requirements" }
   ```

3. **Backend proxies to LLM (Alice never sees this):**
   ```
   POST http://llm-service:8000/kb/query
   Headers: X-API-Key: prod-secret-xyz-12345
   Body: { project_id: "123", query: "authentication requirements" }
   ```

### Setup Instructions

#### LLM Service Configuration

```bash
# llm/.env
LLM_API_KEY=dev-secret-key-12345
GROQ_API_KEY=your-groq-key-here
KB_BASE_DIR=faiss_store
```

#### Backend Service Configuration

```bash
# backend/.env
LLM_SERVICE_URL=http://localhost:8000
LLM_API_KEY=dev-secret-key-12345  # MUST match LLM service
```

#### Backend Integration (PHP/Laravel)

```php
<?php
// app/Services/LLMService.php

namespace App\Services;

use GuzzleHttp\Client;

class LLMService
{
    private $client;
    private $apiKey;
    
    public function __construct()
    {
        $this->client = new Client([
            'base_uri' => env('LLM_SERVICE_URL'),
            'timeout' => 30.0,
        ]);
        $this->apiKey = env('LLM_API_KEY');
    }
    
    public function queryKB($projectId, $query, $topK = 5)
    {
        $response = $this->client->post('/kb/query', [
            'headers' => [
                'X-API-Key' => $this->apiKey,
                'Content-Type' => 'application/json',
            ],
            'json' => [
                'project_id' => $projectId,
                'query' => $query,
                'top_k' => $topK,
            ]
        ]);
        
        return json_decode($response->getBody(), true);
    }
    
    public function buildKB($projectId, $documents, $async = true)
    {
        $response = $this->client->post('/kb/build', [
            'headers' => [
                'X-API-Key' => $this->apiKey,
                'Content-Type' => 'application/json',
            ],
            'json' => [
                'project_id' => $projectId,
                'documents' => $documents,
                'mode' => $async ? 'async' : 'sync',
            ]
        ]);
        
        return json_decode($response->getBody(), true);
    }
}
```

#### Frontend Integration (React)

```javascript
// src/services/llmService.js

// ‚ùå WRONG: Don't call LLM service directly
// const response = await fetch('http://localhost:8000/kb/query', {...});

// ‚úÖ CORRECT: Call YOUR backend
export const queryProjectKB = async (projectId, query) => {
  // Backend will add the LLM API key
  const response = await fetch(`/api/projects/${projectId}/query`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${localStorage.getItem('token')}`, // User's JWT
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ query })
  });
  
  return await response.json();
};
```

### Security Best Practices

**Development:**
- ‚úÖ Use default key: `dev-secret-key-12345`
- ‚úÖ Store in `.env` files (not in code)
- ‚úÖ Frontend never sees the key

**Production:**
- ‚úÖ Generate strong key: `openssl rand -hex 32`
- ‚úÖ Rotate keys every 90 days
- ‚úÖ Use environment-specific keys (staging/prod)
- ‚úÖ LLM service on private network only
- ‚úÖ Enable HTTPS everywhere
- ‚úÖ Add rate limiting
- ‚úÖ Monitor API usage
- ‚úÖ Log all requests

---

## Integration

### Backend Team A: Job Management

**Scenario:** Build knowledge base when user uploads documents

```php
// app/Http/Controllers/DocumentController.php

public function uploadDocument(Request $request, $projectId)
{
    // 1. Store document
    $document = Document::create([
        'project_id' => $projectId,
        'content' => $request->input('content'),
        'type' => $request->input('type'),
    ]);
    
    // 2. Start KB build (async)
    $llmService = new LLMService();
    $result = $llmService->buildKB($projectId, [
        [
            'content' => $document->content,
            'type' => $document->type,
            'meta' => ['document_id' => $document->id]
        ]
    ], true); // async mode
    
    // 3. Store job ID for tracking
    KBJob::create([
        'project_id' => $projectId,
        'job_id' => $result['job_id'],
        'status' => 'queued',
    ]);
    
    return response()->json([
        'message' => 'Document uploaded and KB build started',
        'job_id' => $result['job_id']
    ]);
}

public function checkJobStatus($jobId)
{
    $llmService = new LLMService();
    $jobStatus = $llmService->getJobStatus($jobId);
    
    // Update local database
    KBJob::where('job_id', $jobId)->update([
        'status' => $jobStatus['status'],
        'result' => json_encode($jobStatus['result']),
        'error' => $jobStatus['error'],
    ]);
    
    return response()->json($jobStatus);
}
```

### Backend Team B: Conversation Service with RAG

**Scenario:** Use KB query results as context for AI chat

```php
// app/Services/ConversationService.php

public function generateResponse($projectId, $userMessage, $conversationHistory = [])
{
    // 1. Query KB for relevant context
    $llmService = new LLMService();
    $kbResults = $llmService->queryKB($projectId, $userMessage, 3);
    
    // 2. Build context from KB results
    $context = "Relevant project information:\n";
    foreach ($kbResults['results'] as $result) {
        $context .= "- " . $result['content'] . "\n";
    }
    
    // 3. Build prompt with context
    $prompt = $context . "\n\nConversation:\n";
    foreach ($conversationHistory as $message) {
        $prompt .= $message['role'] . ": " . $message['content'] . "\n";
    }
    $prompt .= "User: " . $userMessage . "\nAssistant: ";
    
    // 4. Call LLM for response (Groq, OpenAI, etc.)
    $aiResponse = $this->callLLM($prompt);
    
    // 5. Store conversation
    Conversation::create([
        'project_id' => $projectId,
        'user_message' => $userMessage,
        'ai_response' => $aiResponse,
        'context_used' => json_encode($kbResults['results']),
    ]);
    
    return [
        'response' => $aiResponse,
        'sources' => $kbResults['results'], // Show user which requirements were referenced
    ];
}
```

### Frontend: Project Dashboard with KB Status

```javascript
// src/components/ProjectDashboard.jsx

import React, { useState, useEffect } from 'react';

const ProjectDashboard = ({ projectId }) => {
  const [kbStatus, setKbStatus] = useState(null);
  const [buildJob, setBuildJob] = useState(null);
  const [loading, setLoading] = useState(true);
  
  // Fetch KB status on mount
  useEffect(() => {
    fetchKBStatus();
  }, [projectId]);
  
  // Poll for job status if building
  useEffect(() => {
    if (buildJob && buildJob.status === 'building') {
      const interval = setInterval(() => {
        checkJobStatus(buildJob.job_id);
      }, 2000);
      
      return () => clearInterval(interval);
    }
  }, [buildJob]);
  
  const fetchKBStatus = async () => {
    try {
      const response = await fetch(`/api/projects/${projectId}/kb/status`, {
        headers: {
          'Authorization': `Bearer ${localStorage.getItem('token')}`
        }
      });
      const data = await response.json();
      setKbStatus(data);
      setLoading(false);
    } catch (error) {
      console.error('Failed to fetch KB status:', error);
      setLoading(false);
    }
  };
  
  const handleBuildKB = async () => {
    try {
      setLoading(true);
      const response = await fetch(`/api/projects/${projectId}/kb/build`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${localStorage.getItem('token')}`,
          'Content-Type': 'application/json',
        }
      });
      const data = await response.json();
      setBuildJob(data);
      setLoading(false);
    } catch (error) {
      console.error('Failed to build KB:', error);
      setLoading(false);
    }
  };
  
  const checkJobStatus = async (jobId) => {
    try {
      const response = await fetch(`/api/jobs/${jobId}`, {
        headers: {
          'Authorization': `Bearer ${localStorage.getItem('token')}`
        }
      });
      const data = await response.json();
      setBuildJob(data);
      
      if (data.status === 'completed') {
        fetchKBStatus(); // Refresh KB status
      }
    } catch (error) {
      console.error('Failed to check job status:', error);
    }
  };
  
  if (loading) return <div>Loading...</div>;
  
  return (
    <div className="project-dashboard">
      <h2>Project: {projectId}</h2>
      
      <div className="kb-status-card">
        <h3>Knowledge Base Status</h3>
        
        {kbStatus?.exists ? (
          <div className="kb-exists">
            <p>‚úÖ Knowledge Base Active</p>
            <p>Version: {kbStatus.version}</p>
            <p>Total Chunks: {kbStatus.total_chunks}</p>
            <p>Last Updated: {new Date(kbStatus.last_built_at).toLocaleString()}</p>
          </div>
        ) : (
          <div className="kb-not-found">
            <p>‚ö†Ô∏è Knowledge Base Not Built</p>
            <button onClick={handleBuildKB}>Build Knowledge Base</button>
          </div>
        )}
        
        {buildJob && buildJob.status === 'building' && (
          <div className="building-indicator">
            <p>üîÑ Building Knowledge Base...</p>
            <p>This may take a few minutes.</p>
          </div>
        )}
        
        {buildJob && buildJob.status === 'completed' && (
          <div className="build-success">
            <p>‚úÖ Build Complete!</p>
            <p>Total Chunks: {buildJob.result.total_chunks}</p>
          </div>
        )}
        
        {buildJob && buildJob.status === 'failed' && (
          <div className="build-error">
            <p>‚ùå Build Failed</p>
            <p>{buildJob.error}</p>
            <button onClick={handleBuildKB}>Retry Build</button>
          </div>
        )}
      </div>
    </div>
  );
};

export default ProjectDashboard;
```

---

## Configuration

### Environment Variables

```bash
# llm/.env

# REQUIRED: Groq API for requirement extraction
GROQ_API_KEY=your-groq-api-key-here

# REQUIRED: API authentication key (shared with backend)
LLM_API_KEY=dev-secret-key-12345

# OPTIONAL: Knowledge Base settings
KB_BASE_DIR=faiss_store
KB_MODEL=all-MiniLM-L6-v2
GROQ_MODEL=moonshotai/kimi-k2-instruct-0905

# OPTIONAL: Embedding settings
EMBEDDING_DIM=384
MAX_CHUNK_SIZE=500
```

### Directory Structure

```
llm/
‚îú‚îÄ‚îÄ main.py                    # FastAPI application
‚îú‚îÄ‚îÄ rag.py                     # RAG manager with FAISS
‚îú‚îÄ‚îÄ build_faiss.py             # Index building utilities
‚îú‚îÄ‚îÄ query_rag.py               # Query utilities
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ .env                       # Configuration (create this)
‚îú‚îÄ‚îÄ .env.example              # Example configuration
‚îú‚îÄ‚îÄ README_KB_API.md          # This documentation
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_api.py
‚îÇ   ‚îî‚îÄ‚îÄ test_kb_api.py
‚îî‚îÄ‚îÄ faiss_store/              # Knowledge bases (auto-created)
    ‚îú‚îÄ‚îÄ proj_123/
    ‚îÇ   ‚îú‚îÄ‚îÄ faiss_index.bin   # FAISS index
    ‚îÇ   ‚îî‚îÄ‚îÄ faiss_meta.pkl    # Metadata with versions
    ‚îî‚îÄ‚îÄ proj_456/
        ‚îú‚îÄ‚îÄ faiss_index.bin
        ‚îî‚îÄ‚îÄ faiss_meta.pkl
```

### Dependencies

```txt
# requirements.txt

# Web Framework
fastapi>=0.104.0
uvicorn[standard]>=0.24.0
python-dotenv>=1.0.0
pydantic>=2.0.0

# LLM Integration
groq>=0.4.0

# Embeddings & Vector Search
sentence-transformers>=2.2.2
faiss-cpu>=1.7.4
numpy>=1.24.0

# Data Processing
pandas>=2.0.0

# File Operations
aiofiles>=23.0.0
python-multipart>=0.0.6

# Testing
pytest>=7.0.0
pytest-asyncio>=0.21.0
httpx>=0.24.0
```

---

## Testing

### Run All Tests

```powershell
cd llm
pytest tests/test_kb_api.py -v
```

### Test Coverage

**Endpoints Tested (6/6 = 100%):**
- ‚úÖ POST /process_document
- ‚úÖ POST /kb/build (sync and async)
- ‚úÖ POST /kb/incremental
- ‚úÖ POST /kb/query
- ‚úÖ GET /kb/status/{project_id}
- ‚úÖ GET /kb/job/{job_id}

**Scenarios Covered (15+ tests):**
- Document processing success
- Document processing errors
- KB building (sync/async modes)
- Empty document handling
- Incremental updates
- Query success and failures
- Status checks (exists/not exists)
- Job tracking (all states)
- Authentication (valid/invalid/missing keys)
- Helper function validation

### Manual Testing

```powershell
# Test health endpoint
curl http://localhost:8000/health

# Test with valid API key
curl -X GET "http://localhost:8000/kb/status/test" `
  -H "X-API-Key: dev-secret-key-12345"

# Test with invalid API key (should fail with 403)
curl -X GET "http://localhost:8000/kb/status/test" `
  -H "X-API-Key: wrong-key"

# Test without API key (should fail with 403)
curl -X GET "http://localhost:8000/kb/status/test"
```

---

## Troubleshooting

### Common Issues

#### 1. "Invalid or missing API key" (403 Forbidden)

**Problem:** Backend not sending API key or using wrong key.

**Solutions:**
- Check `LLM_API_KEY` is set in backend `.env`
- Verify key matches between backend and LLM service
- Ensure header is exactly `X-API-Key` (case-sensitive)
- Restart both services after changing `.env`

```powershell
# Check LLM service .env
cd llm
cat .env | Select-String "LLM_API_KEY"

# Check backend .env
cd ../backend
cat .env | Select-String "LLM_API_KEY"

# Keys must match!
```

#### 2. "GROQ_API_KEY is not set" Warning

**Problem:** Missing Groq API key for requirement extraction.

**Solution:**
- Get free API key from https://console.groq.com/keys
- Add to `llm/.env`: `GROQ_API_KEY=your-key-here`
- Restart service

#### 3. "Knowledge base not found" (404 Error)

**Problem:** Trying to query/update KB before building it.

**Solution:**
- Call `/kb/build` first to create the index
- Verify `project_id` matches exactly
- Check `faiss_store/{project_id}/` directory exists

```powershell
# List all built KBs
ls llm/faiss_store/

# Check specific project
ls llm/faiss_store/proj_123/
```

#### 4. "RAG dependencies not installed" Error

**Problem:** Missing required packages.

**Solution:**
```powershell
cd llm
pip install -r requirements.txt
```

#### 5. Import Errors

**Problem:** Running from wrong directory or missing dependencies.

**Solution:**
```powershell
# Ensure you're in llm/ directory
cd llm

# Verify all dependencies
pip list | Select-String "fastapi|sentence|faiss|groq"

# Reinstall if needed
pip install -r requirements.txt --upgrade
```

#### 6. Frontend Can't Access LLM Endpoints

**Problem:** CORS errors or direct LLM service calls.

**Solution:**
- ‚ùå Frontend should NOT call LLM service directly
- ‚úÖ Frontend calls YOUR backend API
- Backend proxies requests to LLM service
- Backend adds `X-API-Key` header

```javascript
// ‚ùå WRONG
fetch('http://localhost:8000/kb/query', {...})

// ‚úÖ CORRECT
fetch('/api/projects/123/query', {...})
```

#### 7. Job Status Shows "queued" Forever

**Problem:** Async job not actually running (in-memory implementation).

**Solution:**
- Current implementation uses in-memory dict (MVP only)
- For production, use Redis or database for job queue
- Or use sync mode for immediate completion

```json
// Use sync mode instead
{
  "project_id": "proj_123",
  "documents": [...],
  "mode": "sync"  // ‚Üê Wait for completion
}
```

#### 8. High Memory Usage

**Problem:** Large indexes consuming memory.

**Solution:**
- FAISS indexes loaded on demand
- Consider index sharding for large projects
- Monitor memory with: `faiss-cpu` (CPU version) uses less memory than `faiss-gpu`
- Restart service periodically to clear memory

---

## Production Deployment

### Pre-Deployment Checklist

- [ ] Change default API key
- [ ] Set strong `GROQ_API_KEY`
- [ ] Configure production `KB_BASE_DIR`
- [ ] Enable HTTPS
- [ ] Set up LLM service on private network
- [ ] Configure rate limiting
- [ ] Set up monitoring/logging
- [ ] Plan key rotation schedule
- [ ] Test all endpoints
- [ ] Run full test suite

### Production Configuration

```bash
# llm/.env (PRODUCTION)

# Strong API key (generate with: openssl rand -hex 32)
LLM_API_KEY=f4e8a9b2c7d1e6f0a3b8c9d2e7f1a4b8c3d9e2f7a1b6c8d3e9f2a7b1c4d8e3f9

# Groq API key
GROQ_API_KEY=gsk_your_production_key_here

# Production paths
KB_BASE_DIR=/var/lib/llm4reqs/faiss_store

# Production models
KB_MODEL=all-MiniLM-L6-v2
GROQ_MODEL=mixtral-8x7b-32768

# Logging
LOG_LEVEL=INFO
LOG_FILE=/var/log/llm4reqs/app.log
```

### Deployment Steps

```bash
# 1. Pull latest code
git pull origin main

# 2. Navigate to LLM directory
cd llm

# 3. Create virtual environment
python -m venv env
source env/bin/activate  # Linux/Mac
.\env\Scripts\activate   # Windows

# 4. Install dependencies
pip install -r requirements.txt

# 5. Configure environment
cp .env.example .env
nano .env  # Edit with production values

# 6. Run tests
pytest tests/ -v

# 7. Start service (production mode)
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4

# 8. Verify health
curl http://localhost:8000/health
```

### Systemd Service (Linux)

```ini
# /etc/systemd/system/llm4reqs.service

[Unit]
Description=LLM4Reqs Knowledge Base API
After=network.target

[Service]
Type=simple
User=llm4reqs
WorkingDirectory=/opt/llm4reqs/llm
Environment="PATH=/opt/llm4reqs/llm/env/bin"
ExecStart=/opt/llm4reqs/llm/env/bin/uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start service
sudo systemctl enable llm4reqs
sudo systemctl start llm4reqs
sudo systemctl status llm4reqs

# View logs
sudo journalctl -u llm4reqs -f
```

### Docker Deployment

```dockerfile
# Dockerfile

FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create data directory
RUN mkdir -p /data/faiss_store

# Expose port
EXPOSE 8000

# Run application
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```yaml
# docker-compose.yml

version: '3.8'

services:
  llm-service:
    build: ./llm
    ports:
      - "8000:8000"
    environment:
      - LLM_API_KEY=${LLM_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - KB_BASE_DIR=/data/faiss_store
    volumes:
      - ./data/faiss_store:/data/faiss_store
    restart: unless-stopped
    networks:
      - llm4reqs-network

networks:
  llm4reqs-network:
    driver: bridge
```

```bash
# Start with Docker Compose
docker-compose up -d

# View logs
docker-compose logs -f llm-service

# Stop
docker-compose down
```

### Monitoring

```python
# Add to main.py for monitoring

import logging
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/var/log/llm4reqs/app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Log all API calls
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = datetime.now()
    logger.info(f"Request: {request.method} {request.url.path}")
    
    response = await call_next(request)
    
    duration = (datetime.now() - start_time).total_seconds()
    logger.info(f"Response: {response.status_code} - Duration: {duration}s")
    
    return response
```

### Scaling Considerations

**Current Limitations (MVP):**
- In-memory job tracking (use Redis for production)
- Single-server deployment
- No load balancing

**Production Recommendations:**
- Use Redis for distributed job queue
- Deploy multiple workers behind load balancer
- Use database for persistent job tracking
- Implement caching layer
- Add CDN for static assets
- Set up auto-scaling

---

## Quick Reference

### Command Cheat Sheet

```powershell
# Development
cd llm
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Testing
pytest tests/test_kb_api.py -v
curl http://localhost:8000/health

# Build KB from CSV
python build_faiss.py --csv data/enriched_requirements.csv --project-id proj_123

# Check API documentation
# Open http://localhost:8000/docs in browser
```

### HTTP Status Codes

| Code | Meaning | When It Happens |
|------|---------|-----------------|
| 200 | Success | Request completed successfully |
| 400 | Bad Request | Invalid parameters or missing required fields |
| 403 | Forbidden | Invalid or missing API key |
| 404 | Not Found | Knowledge base doesn't exist for project |
| 500 | Server Error | Internal error (check logs) |

### Key Files

| File | Purpose |
|------|---------|
| `llm/main.py` | FastAPI application with all endpoints |
| `llm/rag.py` | RAG manager with FAISS operations |
| `llm/build_faiss.py` | Index building utilities |
| `llm/requirements.txt` | Python dependencies |
| `llm/.env` | Configuration (YOU create this) |
| `llm/tests/test_kb_api.py` | Unit tests |
| `llm/faiss_store/` | Knowledge base storage (auto-created) |

---

## Support

### Documentation
- **API Reference:** This document
- **FastAPI Docs:** http://localhost:8000/docs (auto-generated)
- **Source Code:** Check inline comments in `main.py`, `rag.py`

### Getting Help
1. Check troubleshooting section above
2. Review logs: `uvicorn` terminal output
3. Run tests: `pytest tests/test_kb_api.py -v`
4. Check FastAPI auto-docs: http://localhost:8000/docs

---

## Summary

**What You Built:**
- ‚úÖ 6 REST API endpoints for knowledge base management
- ‚úÖ Project-specific FAISS vector search
- ‚úÖ Incremental updates with versioning
- ‚úÖ Async and sync build modes
- ‚úÖ API key authentication
- ‚úÖ Complete test suite (15+ tests)
- ‚úÖ Thread-safe operations

**What Teams Can Do:**
- **Backend Team A:** Integrate job management and document processing
- **Backend Team B:** Use RAG for conversation context
- **Frontend Teams:** Display KB status and build progress
- **DevOps:** Deploy to staging/production

**Time Investment:**
- Setup: 5 minutes
- First KB build: 1 minute
- Integration: 1-2 hours per team
- **Ready to use!** ‚úÖ

---

**Version:** 1.0.0  
**Last Updated:** October 20, 2025  
**Status:** ‚úÖ Production Ready
